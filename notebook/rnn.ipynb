{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f38ebda-7a39-4651-89de-542f7cc11340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pysrt\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "import optuna\n",
    "\n",
    "#optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f754cd3-3022-4dfb-a38e-a358d700bcc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Анализ таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec8fcf2-e7f1-4844-a335-659aee64e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('table_pazzle.csv', dtype=str, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742aac48-87f7-486d-9026-90ea764d0b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3504, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fddda771-e376-459b-bcb4-27f443108606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "middle    1831\n",
       "easy      1105\n",
       "hard       568\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e281b333-9b26-42f1-a444-7d365162be5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3461, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.file_name.isin(error_file) == False]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b719ab9-fca2-403b-a7b0-eeb4bbcfa08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "middle    1812\n",
       "easy      1088\n",
       "hard       561\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fc84708-44df-4e94-89b8-ba7909087da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "middle    0.523548\n",
       "easy      0.314360\n",
       "hard      0.162092\n",
       "Name: level, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['level'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cca0f0f9-20b0-4092-a7af-4c17f750fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c31b12-2a45-4a13-9e00-a4c43545568e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Токенезируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631ab7e8-583c-4032-810e-c9a03279eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_table.csv', dtype=str, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513c6694-ac85-4d91-9e0f-c537d80a944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test(df):\n",
    "    index_test = []\n",
    "    for level_name in df['level'].unique():\n",
    "        len_level = df[df['level'] == level_name].shape[0]\n",
    "        index_test.extend(list((df['level'] == level_name).sample(int(len_level*20/100)).index))\n",
    "    df_train = df.drop(index_test).copy()\n",
    "    df_test = df.loc[index_test].copy()\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffd19e87-dde9-4259-85da-f87d169df807",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = make_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda4d305-6fb4-42fa-9a07-d296cfbe3a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(middle    1454\n",
       " easy       903\n",
       " hard       457\n",
       " Name: level, dtype: int64,\n",
       " middle    377\n",
       " easy      200\n",
       " hard      114\n",
       " Name: level, dtype: int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['level'].value_counts(), df_test['level'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68c47b65-c1cc-4fe4-8b28-62f1fd5df60f",
   "metadata": {},
   "source": [
    "# без стоп-слов\n",
    "def tokens_df(srt, tokenizer, stop_words):\n",
    "    tokens_list = []\n",
    "    srt = [string.text for string in srt]\n",
    "    for string in tokenizer.tokenizer.pipe(srt, batch_size=2000):\n",
    "        tokens = tokenizer(string)\n",
    "        tokens_list.extend([str(token).lower() for token in tokens \\\n",
    "                            if token.is_alpha and str(token).lower() not in stop_words])\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d99aeef-435b-48f3-ad3c-898587bba66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# со стоп-словами\n",
    "def tokens_df(srt, tokenizer, stop_words):\n",
    "    tokens_list = []\n",
    "    srt = [string.text for string in srt]\n",
    "    for string in tokenizer.tokenizer.pipe(srt, batch_size=2000):\n",
    "        tokens = tokenizer(string)\n",
    "        tokens_list.extend([str(token).lower() for token in tokens \\\n",
    "                            if token.is_alpha])\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c121d6db-e3ad-4372-b37e-4037417a3643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "nlp = English()\n",
    "num = 0\n",
    "pbar = tqdm(df_train['file_name'], leave=False)\n",
    "level = {'easy':0, 'middle':1, 'hard':2}\n",
    "\n",
    "train_list = []\n",
    "counter_words = Counter()\n",
    "counter_label = Counter()\n",
    "len_list = []\n",
    "\n",
    "for index, name in enumerate(pbar):\n",
    "    num+= 1\n",
    "    label = level[df_train.iloc[index]['level']]\n",
    "    name_film = df_train.iloc[index]['name']\n",
    "    \n",
    "    srt = pysrt.open(f'sub/{name}', encoding='iso-8859-1')\n",
    "    tokens = tokens_df(srt, nlp, stop_words)\n",
    "    \n",
    "    len_list.append(len(tokens))\n",
    "    counter_words.update(tokens)\n",
    "    counter_label.update(str(label))\n",
    "    \n",
    "    train_list.append((tokens, label, name_film))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d59806d5-16d6-439b-b275-475ff15883eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122212"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31af8cbf-7023-4b68-8211-3c7b16e45701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1631edd-ea4a-44c3-a02d-0c7289b351bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(df_test['file_name'], leave=False)\n",
    "\n",
    "test_list = []\n",
    "\n",
    "for index, name in enumerate(pbar):\n",
    "    label = level[df_test.iloc[index]['level']]\n",
    "    name_film = df_test.iloc[index]['name']\n",
    "    \n",
    "    srt = pysrt.open(f'sub/{name}', encoding='iso-8859-1')\n",
    "    tokens = tokens_df(srt, nlp, stop_words)\n",
    "    test_list.append((tokens, label, name_film))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d5af9af-47da-46e8-884f-2202c4759c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_list' (list)\n",
      "Stored 'test_list' (list)\n",
      "Stored 'counter_words' (Counter)\n",
      "Stored 'counter_label' (Counter)\n",
      "Stored 'len_list' (list)\n"
     ]
    }
   ],
   "source": [
    "%store train_list\n",
    "%store test_list\n",
    "%store counter_words\n",
    "%store counter_label\n",
    "%store len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f273a34f-e5d4-46a1-a7c4-a669bef1c1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(len_list).hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90dfa0eb-415f-4e22-b910-92ef0e0f06c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24748"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(len_list).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97df1049-a1b8-4c13-9782-20569a1acc60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7972.0\n",
      "8759.0\n",
      "10157.0\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(len_list).quantile(0.5))\n",
    "print(pd.Series(len_list).quantile(0.6))\n",
    "print(pd.Series(len_list).quantile(0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dedcb015-ce2a-4067-9838-f91bb93046ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words = [i[0] for i in train_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c77ce-4cba-4c01-a93e-a45054bb9eb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Обучаем на эмбединге"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36685d8-8f4b-4ae9-a5b2-fff828a38215",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Загружаем word2vec модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cfe41e0-ef2a-42d7-9152-497371072e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a923df-cf37-4484-a77d-99777f866ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(\"models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84007345-8400-4f07-a2bc-80ff03ce839a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Формируем батчи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39637e70-dc71-45e7-a464-c4b6a366dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_string(list_result, index_dict, LEN_STRI):\n",
    "    data = []\n",
    "    for tokens, label, name in list_result:\n",
    "        # прибавляем 2, так как при создании эмбеддингов это 0 pad, 1 unknow\n",
    "        stri = list(map(lambda x: index_dict.get(x, -1)+2, tokens[:LEN_STRI]))\n",
    "        stri = stri + [0]*(LEN_STRI - len(stri)) if len(stri) < LEN_STRI else stri\n",
    "        data.append((torch.LongTensor(stri), label, name)\n",
    "                   )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ea9b666-d9d9-4b03-a197-3d9b7fb6080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_STRI = 2000\n",
    "train_data = trans_string(train_list, wv.key_to_index, LEN_STRI)\n",
    "test_data = trans_string(test_list, wv.key_to_index, LEN_STRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a503f34d-f7bd-4cd3-906d-eb8d935de2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c573429b-dceb-4032-9946-53d12ebe6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "463f5a16-6a60-4b31-9280-99c6185a4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, \n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3524a-4461-4153-a207-1a5803c11579",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Создаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19abc2-d56e-4315-942d-6a6bac827303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_string(list_result, index_dict, LEN_STRI):\n",
    "    data = []\n",
    "    for tokens, label, name in list_result:\n",
    "        # прибавляем 2, так как при создании эмбеддингов это 0 pad, 1 unknow\n",
    "        stri = list(map(lambda x: index_dict.get(x, -1)+2, tokens[:LEN_STRI]))\n",
    "        stri = stri + [0]*(LEN_STRI - len(stri)) if len(stri) < LEN_STRI else stri\n",
    "        data.append((torch.LongTensor(stri), label, name)\n",
    "                   )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e86fff-9c6f-460f-9d61-ffe2ca3539b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_STRI = 2000\n",
    "train_data = trans_string(train_list, wv.key_to_index, LEN_STRI)\n",
    "test_data = trans_string(test_list, wv.key_to_index, LEN_STRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd778fa-d07e-4845-9956-8492a97e41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, \n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ceaec4-72af-41b4-b99e-292fde270c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "                \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           n_layers, bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        hidden_dim = hidden_dim*n_layers if bidirectional == False else hidden_dim*n_layers*2\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(), \n",
    "                                nn.Linear(hidden_dim, int(hidden_dim/2)),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(int(hidden_dim/2), output_dim)\n",
    "                               )\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        fc = self.fc(hidden)\n",
    "            \n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd6322e9-be0a-4204-9393-774dcb7cab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = wv['cat'].shape[0]\n",
    "output_dim = len(counter_label)\n",
    "n_layers = 1\n",
    "bidirectional = True\n",
    "dropout = 0\n",
    "pad_idx = 0\n",
    "model = RNNBaseline(len(wv.key_to_index)+2, embed_dim, \n",
    "                    embed_dim, output_dim, n_layers, \n",
    "                   bidirectional, dropout, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e20ed49-a3cd-4019-9649-233a6e4b8912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27916/115841204.py:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525539683/work/torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  model.embedding.weight[idx+2] = torch.Tensor(wv.get_vector(word))\n"
     ]
    }
   ],
   "source": [
    "# меняем веса у эмбединга как у word2vec \n",
    "with torch.no_grad():\n",
    "    for word, idx in wv.key_to_index.items():\n",
    "        model.embedding.weight[idx+2] = torch.Tensor(wv.get_vector(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4f4a709-beae-45f8-ac2e-87d1b29d23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_embeddings(model, req_grad=False):\n",
    "    embeddings = model.embedding\n",
    "    for c_p in embeddings.parameters():\n",
    "        c_p.requires_grad = req_grad\n",
    "freeze_embeddings(model, req_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2bd5e17-c8fe-47f8-8f04-181b73be8429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(3000002, 300, padding_idx=0)\n",
       "  (rnn): LSTM(300, 300, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=300, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6919cc4e-ffd3-4e1d-abd3-f55ac0cb1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, data in enumerate(train_loader):\n",
    "            # print(data[0].shape)\n",
    "            # print(data[1].shape)\n",
    "            # input()\n",
    "            \n",
    "            x_gpu = data[0].to(device)\n",
    "            y_gpu = data[1].to(device)\n",
    "            prediction = model(x_gpu)  \n",
    "            \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y_gpu.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "        \n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(f'Average loss:{ave_loss}, train_accuracy: {round(train_accuracy, 2)},\\\n",
    "              val_accuracy: {round(val_accuracy, 2)}')\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "\n",
    "    predict_list = []\n",
    "    y_list = []\n",
    "    model.eval() # Evaluation mode\n",
    "    for data in loader:\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        prediction = model(X)\n",
    "        _, index = torch.max(prediction, 1)\n",
    "        predict_list.extend(list(index.cpu()))\n",
    "        y_list.extend(list(y.cpu()))\n",
    "        \n",
    "    accuracy = float(sum([pred == gt for pred, gt in zip(predict_list, y_list)])) / len(predict_list)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f51f1441-46ba-461b-a916-f3e25bfc7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'params': model.rnn.parameters()},\n",
    "              {'params': model.fc.parameters()}\n",
    "              ]\n",
    "\n",
    "opt = torch.optim.Adam(parameters)\n",
    "loss_func = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee65180d-74c0-44f6-8c0f-b5d63db07eca",
   "metadata": {},
   "source": [
    "!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aaa652fc-0734-4592-9318-f99da8e8cff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:1.0357723236083984, train_accuracy: 0.52,              val_accuracy: 0.52\n",
      "Average loss:0.9997428059577942, train_accuracy: 0.53,              val_accuracy: 0.51\n",
      "Average loss:0.9713132977485657, train_accuracy: 0.55,              val_accuracy: 0.46\n",
      "Average loss:0.9087525010108948, train_accuracy: 0.59,              val_accuracy: 0.49\n",
      "Average loss:0.8320738077163696, train_accuracy: 0.64,              val_accuracy: 0.49\n",
      "Average loss:0.6983506083488464, train_accuracy: 0.72,              val_accuracy: 0.48\n",
      "Average loss:0.5580732226371765, train_accuracy: 0.79,              val_accuracy: 0.44\n",
      "Average loss:0.41863834857940674, train_accuracy: 0.85,              val_accuracy: 0.44\n",
      "Average loss:0.29247087240219116, train_accuracy: 0.9,              val_accuracy: 0.46\n",
      "Average loss:0.2100418508052826, train_accuracy: 0.93,              val_accuracy: 0.45\n",
      "Average loss:0.14015516638755798, train_accuracy: 0.95,              val_accuracy: 0.44\n",
      "Average loss:0.1218351349234581, train_accuracy: 0.96,              val_accuracy: 0.44\n",
      "Average loss:0.08283901959657669, train_accuracy: 0.97,              val_accuracy: 0.46\n",
      "Average loss:0.06881789118051529, train_accuracy: 0.98,              val_accuracy: 0.46\n",
      "Average loss:0.054925043135881424, train_accuracy: 0.98,              val_accuracy: 0.45\n",
      "Average loss:0.053231049329042435, train_accuracy: 0.98,              val_accuracy: 0.44\n",
      "Average loss:0.0452411025762558, train_accuracy: 0.98,              val_accuracy: 0.45\n",
      "Average loss:0.039878956973552704, train_accuracy: 0.98,              val_accuracy: 0.43\n",
      "Average loss:0.030873456969857216, train_accuracy: 0.98,              val_accuracy: 0.44\n",
      "Average loss:0.02877792902290821, train_accuracy: 0.98,              val_accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history, val_history \\\n",
    "= train_model(model, train_loader, val_loader, loss_func, opt, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa73199-2b54-4296-b686-18d1f1cecabe",
   "metadata": {},
   "source": [
    "# Формируем эмбединги с нуля "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abd51977-280e-4a50-a208-dbe2ba32e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {word:ind+2 for ind, word in enumerate(counter_words.keys())}\n",
    "index_word['<PAD>'] = 0\n",
    "index_word['<unknow>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ff9a9f1-d2f0-4315-a78b-b35bb74fc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_string(list_result, index_dict, LEN_STRI):\n",
    "    data = []\n",
    "    for tokens, label, name in list_result:\n",
    "        # прибавляем 2, так как при создании эмбеддингов это 0 pad, 1 unknow\n",
    "        stri = list(map(lambda x: index_dict.get(x, 1), tokens[:LEN_STRI]))\n",
    "        stri = stri + [0]*(LEN_STRI - len(stri)) if len(stri) < LEN_STRI else stri\n",
    "        data.append((torch.LongTensor(stri), label, name)\n",
    "                   )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3a56b52-6cb1-49bb-8df7-d59ac49ca50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_STRI = 150\n",
    "train_data = trans_string(train_list, index_word, LEN_STRI)\n",
    "test_data = trans_string(test_list, index_word, LEN_STRI)\n",
    "shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e014b0bd-8557-44b7-b78d-3247aec92ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, \n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_data, batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5b2408-00a8-4bf5-bc8c-2a693dcc5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "                \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           n_layers, bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        hidden_dim = hidden_dim*n_layers if bidirectional == False else hidden_dim*n_layers*2\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(), \n",
    "                                nn.Linear(hidden_dim, int(hidden_dim/2)),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(int(hidden_dim/2), output_dim)\n",
    "                               )\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        fc = self.fc(hidden)\n",
    "            \n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc3cc5ac-a005-43c9-bb2b-be61dadd35cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "output_dim = len(counter_label)\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0\n",
    "pad_idx = 0\n",
    "model = RNNBaseline(len(index_word), embed_dim, \n",
    "                    embed_dim, output_dim, n_layers, \n",
    "                   bidirectional, dropout, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28c2d38c-67b1-42ed-8ae1-9e2989ee359d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(122214, 100, padding_idx=0)\n",
       "  (rnn): LSTM(100, 100, num_layers=2, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=200, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6088ab4-9c86-4d72-bb98-540028fc7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())\n",
    "loss_func = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa65ea4-beae-44c4-a207-46a40890e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, data in enumerate(train_loader):\n",
    "            # print(data[0].shape)\n",
    "            # print(data[1].shape)\n",
    "            # input()\n",
    "            \n",
    "            x_gpu = data[0].to(device)\n",
    "            y_gpu = data[1].to(device)\n",
    "            prediction = model(x_gpu)  \n",
    "            \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y_gpu.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "        \n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(f'Average loss:{ave_loss}, train_accuracy: {round(train_accuracy, 2)},\\\n",
    "              val_accuracy: {round(val_accuracy, 2)}')\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "\n",
    "    predict_list = []\n",
    "    y_list = []\n",
    "    model.eval() # Evaluation mode\n",
    "    for data in loader:\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        prediction = model(X)\n",
    "        _, index = torch.max(prediction, 1)\n",
    "        predict_list.extend(list(index.cpu()))\n",
    "        y_list.extend(list(y.cpu()))\n",
    "        \n",
    "    accuracy = float(sum([pred == gt for pred, gt in zip(predict_list, y_list)])) / len(predict_list)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bb1c3602-55f6-4787-aa6a-84b389145cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss:1.0374284982681274, train_accuracy: 0.52,              val_accuracy: 0.55\n",
      "Average loss:0.9894651174545288, train_accuracy: 0.53,              val_accuracy: 0.55\n",
      "Average loss:0.8946910500526428, train_accuracy: 0.58,              val_accuracy: 0.42\n",
      "Average loss:0.697466254234314, train_accuracy: 0.71,              val_accuracy: 0.43\n",
      "Average loss:0.4155140817165375, train_accuracy: 0.84,              val_accuracy: 0.45\n",
      "Average loss:0.22731009125709534, train_accuracy: 0.93,              val_accuracy: 0.42\n",
      "Average loss:0.12437214702367783, train_accuracy: 0.97,              val_accuracy: 0.43\n",
      "Average loss:0.0937967374920845, train_accuracy: 0.98,              val_accuracy: 0.44\n",
      "Average loss:0.07488717883825302, train_accuracy: 0.98,              val_accuracy: 0.42\n",
      "Average loss:0.05567287653684616, train_accuracy: 0.98,              val_accuracy: 0.47\n",
      "Average loss:0.04208597168326378, train_accuracy: 0.98,              val_accuracy: 0.44\n",
      "Average loss:0.037755709141492844, train_accuracy: 0.98,              val_accuracy: 0.46\n",
      "Average loss:0.035500336438417435, train_accuracy: 0.98,              val_accuracy: 0.44\n",
      "Average loss:0.028340667486190796, train_accuracy: 0.98,              val_accuracy: 0.44\n",
      "Average loss:0.02758660726249218, train_accuracy: 0.98,              val_accuracy: 0.43\n",
      "Average loss:0.02545943111181259, train_accuracy: 0.98,              val_accuracy: 0.43\n",
      "Average loss:0.025037681683897972, train_accuracy: 0.98,              val_accuracy: 0.45\n",
      "Average loss:0.023388879373669624, train_accuracy: 0.98,              val_accuracy: 0.46\n",
      "Average loss:0.022434594109654427, train_accuracy: 0.98,              val_accuracy: 0.45\n",
      "Average loss:0.022584298625588417, train_accuracy: 0.98,              val_accuracy: 0.43\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history, val_history \\\n",
    "= train_model(model, train_loader, val_loader, loss_func, opt, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dad0e9-d1b4-4ba2-a6ce-fda10a7a34ef",
   "metadata": {},
   "source": [
    "# Пробуем увеличить количество данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7de0d6ce-9f2a-46d4-88e7-957db5c0abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {word:ind+2 for ind, word in enumerate(counter_words.keys())}\n",
    "index_word['<PAD>'] = 0\n",
    "index_word['<unknow>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5bcdad2e-f5bf-4499-a2ec-cbf6e4dd2029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_string(list_result, index_dict, LEN_STRI):\n",
    "    data = []\n",
    "    for tokens, label, name in list_result:\n",
    "        for index_start, index_end in zip(range(0, len(tokens), LEN_STRI), \n",
    "                                          range(LEN_STRI, len(tokens)+LEN_STRI, LEN_STRI)):\n",
    "        \n",
    "            stri = list(map(lambda x: index_dict.get(x, 1), tokens[index_start:index_end]))\n",
    "            stri = stri + [0]*(LEN_STRI - len(stri)) if len(stri) < LEN_STRI else stri\n",
    "            data.append((torch.LongTensor(stri), label, name)\n",
    "                       )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8cba4398-d8dd-483a-b1ea-cc9efd430d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEN_STRI = 100\n",
    "train_data_many = trans_string(train_list, index_word, LEN_STRI)\n",
    "test_data_many = trans_string(test_list, index_word, LEN_STRI)\n",
    "shuffle(train_data_many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "44c1f371-c3ce-4d91-99a1-5d84f5668b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_many' (list)\n",
      "Stored 'test_data_many' (list)\n"
     ]
    }
   ],
   "source": [
    "%store train_data_many\n",
    "%store test_data_many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5b4758b6-b12b-4d91-a554-8f29744d7b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138633"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e54e64d7-0328-4cb1-9550-24898e714d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_many[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fed60968-679f-4009-9267-1185bb368d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = 0 \n",
    "one = 0\n",
    "two = 0\n",
    "qw = set()\n",
    "for i in range(len(train_data_many)):\n",
    "    qw.add(len(train_data_many[i][0]))\n",
    "    if train_data_many[i][1] == 0:\n",
    "        zero += 1\n",
    "    elif train_data_many[i][1] == 1:\n",
    "        one += 1\n",
    "    elif train_data_many[i][1] == 2:\n",
    "        two += 1\n",
    "    else: \n",
    "        print('ошибка')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "491f86b1-df1b-4408-9a00-a61c71b47449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a07c16d7-1236-4efb-a542-c65cd5acb392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70709, 122142, 37962)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero, one, two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c27bc7ed-c20e-4014-8728-b64f735f2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data_many, batch_size=64, \n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test_data_many, batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a3ba87-8f34-49a9-8329-4104e7d383f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "                \n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                           n_layers, bidirectional=bidirectional,\n",
    "                           dropout=dropout)\n",
    "        \n",
    "        hidden_dim = hidden_dim*n_layers if bidirectional == False else hidden_dim*n_layers*2\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Flatten(), \n",
    "                                nn.Linear(hidden_dim, int(hidden_dim/2)),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(int(hidden_dim/2), output_dim)\n",
    "                               )\n",
    "        \n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        fc = self.fc(hidden)\n",
    "            \n",
    "        return fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cd52899a-d106-4a3b-96da-bf7e5ac9ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 50\n",
    "output_dim = len(counter_label)\n",
    "hidden_dim = 100\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "pad_idx = 0\n",
    "model = RNNBaseline(len(index_word), embed_dim, \n",
    "                    hidden_dim, output_dim, n_layers, \n",
    "                   bidirectional, dropout, pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b94bc492-f088-4c9a-ad45-639a8a693513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(122214, 50, padding_idx=0)\n",
       "  (rnn): LSTM(50, 100, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=200, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5e9f30b3-82ca-4c77-a1c1-7387c16d83f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_func = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad8614f-2e26-4716-8742-f1e8a459970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, data in enumerate(train_loader):\n",
    "            # print(data[0].shape)\n",
    "            # print(data[1].shape)\n",
    "            # input()\n",
    "            \n",
    "            x_gpu = data[0].to(device)\n",
    "            y_gpu = data[1].to(device)\n",
    "            prediction = model(x_gpu)  \n",
    "            \n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y_gpu)\n",
    "            total_samples += y_gpu.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "        \n",
    "        ave_loss = loss_accum / i_step\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(f'{epoch}. Average loss:{ave_loss}, train_accuracy: {round(train_accuracy, 2)},\\\n",
    "              val_accuracy: {round(val_accuracy, 2)}')\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "\n",
    "    predict_list = []\n",
    "    y_list = []\n",
    "    model.eval() # Evaluation mode\n",
    "    for data in loader:\n",
    "        X = data[0].to(device)\n",
    "        y = data[1].to(device)\n",
    "        prediction = model(X)\n",
    "        _, index = torch.max(prediction, 1)\n",
    "        predict_list.extend(list(index.cpu()))\n",
    "        y_list.extend(list(y.cpu()))\n",
    "        \n",
    "    accuracy = float(sum([pred == gt for pred, gt in zip(predict_list, y_list)])) / len(predict_list)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7eb46e5c-9957-453e-ad45-6d2041ad34cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:0.9968255758285522, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "1. Average loss:0.9865835905075073, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9715284705162048, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9535956382751465, train_accuracy: 0.55,              val_accuracy: 0.55\n",
      "4. Average loss:0.9370211362838745, train_accuracy: 0.56,              val_accuracy: 0.56\n",
      "5. Average loss:0.9214831590652466, train_accuracy: 0.57,              val_accuracy: 0.56\n",
      "6. Average loss:0.9063810110092163, train_accuracy: 0.58,              val_accuracy: 0.54\n",
      "7. Average loss:0.8903160095214844, train_accuracy: 0.59,              val_accuracy: 0.56\n",
      "8. Average loss:0.874950110912323, train_accuracy: 0.6,              val_accuracy: 0.55\n",
      "9. Average loss:0.8598862886428833, train_accuracy: 0.61,              val_accuracy: 0.55\n",
      "10. Average loss:0.8444107174873352, train_accuracy: 0.62,              val_accuracy: 0.55\n",
      "11. Average loss:0.8290818929672241, train_accuracy: 0.63,              val_accuracy: 0.54\n",
      "12. Average loss:0.8134743571281433, train_accuracy: 0.64,              val_accuracy: 0.53\n",
      "13. Average loss:0.7984012365341187, train_accuracy: 0.65,              val_accuracy: 0.52\n",
      "14. Average loss:0.7828993201255798, train_accuracy: 0.66,              val_accuracy: 0.54\n",
      "15. Average loss:0.7677887678146362, train_accuracy: 0.66,              val_accuracy: 0.52\n",
      "16. Average loss:0.7526134848594666, train_accuracy: 0.67,              val_accuracy: 0.53\n",
      "17. Average loss:0.7372862100601196, train_accuracy: 0.68,              val_accuracy: 0.5\n",
      "18. Average loss:0.7220577001571655, train_accuracy: 0.69,              val_accuracy: 0.53\n",
      "19. Average loss:0.707655668258667, train_accuracy: 0.7,              val_accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "loss_history, train_history, val_history \\\n",
    "= train_model(model, train_loader, val_loader, loss_func, opt, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8095c689-257b-47c5-b5ca-d2283d95f259",
   "metadata": {},
   "source": [
    "Удалось уменьшить скорость переобучения, но не остановаить его. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5e4d6-b029-45af-8b88-bd0100ea8d5e",
   "metadata": {},
   "source": [
    "# Попробуем использовать optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120689a8-6016-4f3c-940b-e24606222b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2c14e6-9289-44ca-82f4-c9ae867cdce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {word:ind+2 for ind, word in enumerate(counter_words.keys())}\n",
    "index_word['<PAD>'] = 0\n",
    "index_word['<unknow>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7114f8d6-5161-439e-ab15-28ba7026f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_string(list_result, index_dict, LEN_STRI):\n",
    "    data = []\n",
    "    for tokens, label, name in list_result:\n",
    "        for index_start, index_end in zip(range(0, len(tokens), LEN_STRI), \n",
    "                                          range(LEN_STRI, len(tokens)+LEN_STRI, LEN_STRI)):\n",
    "        \n",
    "            stri = list(map(lambda x: index_dict.get(x, 1), tokens[index_start:index_end]))\n",
    "            stri = stri + [0]*(LEN_STRI - len(stri)) if len(stri) < LEN_STRI else stri\n",
    "            data.append((torch.LongTensor(stri), label, name)\n",
    "                       )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dfcf826-d762-41c1-9f2e-78f4c88f94eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(train_list, test_list, index_word, LEN_STRI):\n",
    "    #LEN_STRI = 100\n",
    "    train_data_many = trans_string(train_list, index_word, LEN_STRI)\n",
    "    test_data_many = trans_string(test_list, index_word, LEN_STRI)\n",
    "    shuffle(train_data_many)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data_many, batch_size=64, \n",
    "                                               shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(test_data_many, batch_size=64, \n",
    "                                               shuffle=True)    \n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ede6611d-9059-41dc-bdee-66fcedadd403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(vocab_size, embed_dim, hidden_dim, output_dim,\n",
    "               n_layers, bidirectional, dropout):\n",
    "    # vocab_size = len(index_word)\n",
    "    # embed_dim = 50\n",
    "    # output_dim = len(counter_label)\n",
    "    # hidden_dim = 100\n",
    "    # n_layers = 2\n",
    "    # bidirectional = True\n",
    "    # dropout = 0.3\n",
    "    # pad_idx = 0\n",
    "    model = RNNBaseline(vocab_size, embed_dim, \n",
    "                        embed_dim, output_dim, n_layers, \n",
    "                       bidirectional, dropout, pad_idx=0)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "559d05c9-45a4-45e1-92bf-1b1cdbbd0128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluet(param):\n",
    "    global train_list, test_list, index_word, counter_label\n",
    "    \n",
    "    train_loader, val_loader = make_batch(train_list, test_list, \n",
    "                                          index_word, param['LEN_STRI'])\n",
    "    model = make_model(len(index_word), 2, param['hidden_dim'], \n",
    "                       len(counter_label), param['n_layers'], True, \n",
    "                       param['dropout'])\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=param['lr'])\n",
    "    loss_func = nn.CrossEntropyLoss().type(torch.cuda.FloatTensor)\n",
    "    loss_history, train_history, val_history \\\n",
    "    = train_model(model, train_loader, val_loader, loss_func, opt, param['epochs'])   \n",
    "    \n",
    "    return val_history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbdc8f1c-42ea-4e3e-a7a9-4acb144164c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    global history \n",
    "     \n",
    "    params = {\n",
    "          'lr': trial.suggest_float('learning_rate', 1.5e-5, 1.5e-5),\n",
    "          'epochs': trial.suggest_int(\"epochs\", 25, 50),\n",
    "          'LEN_STRI': trial.suggest_int(\"LEN_STRI\", 50, 250), \n",
    "          #'embed_dim': trial.suggest_int(\"embed_dim\", 10, 250), \n",
    "          'hidden_dim': trial.suggest_int(\"hidden_dim\", 10, 250),\n",
    "          'n_layers': trial.suggest_int(\"n_layers\", 1, 3),\n",
    "          'dropout': trial.suggest_float(\"dropout\", 0, 0.4),\n",
    "          }\n",
    "\n",
    "    accuracy = train_and_evaluet(params)\n",
    "    history.append((params, accuracy))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08dea86-f255-40cc-903a-27426820df20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5e-05"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d2e1bac-21ca-46cf-b765-975d7119645e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-02-16 10:15:48,893]\u001b[0m A new study created in memory with name: no-name-025d568a-012a-40c4-8cfd-7216921db53c\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3136ca37ef074805a4250e2bbbff458b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.1904152631759644, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.1616343259811401, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "2. Average loss:1.1313540935516357, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "3. Average loss:1.1011836528778076, train_accuracy: 0.31,              val_accuracy: 0.29\n",
      "4. Average loss:1.0738098621368408, train_accuracy: 0.47,              val_accuracy: 0.56\n",
      "5. Average loss:1.052178144454956, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:1.0361586809158325, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:1.0248600244522095, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:1.0164583921432495, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:1.010385274887085, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:1.0058926343917847, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:1.0028996467590332, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:1.0006135702133179, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9992174506187439, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9983137845993042, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9977287650108337, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9972646832466125, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9971659183502197, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.996910572052002, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9969256520271301, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9968090057373047, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9968560934066772, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9968478083610535, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9968221187591553, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9967691898345947, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9966995716094971, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9967663288116455, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9967349171638489, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9966831803321838, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9967526793479919, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9967287182807922, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9966934323310852, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9967758655548096, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9967442750930786, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9967421889305115, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.996705949306488, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.9967262744903564, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.9967129230499268, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.9967759847640991, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "39. Average loss:0.9967623353004456, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "40. Average loss:0.9967734813690186, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "41. Average loss:0.9966519474983215, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "42. Average loss:0.9967174530029297, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "43. Average loss:0.9966460466384888, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "44. Average loss:0.9966658353805542, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "45. Average loss:0.9966821074485779, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "46. Average loss:0.996728241443634, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "47. Average loss:0.9966052770614624, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "48. Average loss:0.996654212474823, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 10:33:09,239]\u001b[0m Trial 0 finished with value: 0.5607067137809187 and parameters: {'learning_rate': 1.5e-05, 'epochs': 49, 'LEN_STRI': 202, 'hidden_dim': 14, 'n_layers': 2, 'dropout': 0.38161229182870404}. Best is trial 0 with value: 0.5607067137809187.\u001b[0m\n",
      "0. Average loss:1.140103816986084, train_accuracy: 0.17,              val_accuracy: 0.18\n",
      "1. Average loss:1.1071285009384155, train_accuracy: 0.33,              val_accuracy: 0.55\n",
      "2. Average loss:1.078306794166565, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:1.0536959171295166, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:1.0337218046188354, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:1.0183089971542358, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:1.0078843832015991, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:1.001959204673767, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9992494583129883, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9982026219367981, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9977614879608154, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.997584342956543, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9974933862686157, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9974072575569153, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9973008632659912, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9973455667495728, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9973565936088562, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9972946047782898, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9973053932189941, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.997160017490387, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9972254633903503, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9971800446510315, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9972113966941833, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9971420764923096, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9970431327819824, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9970957636833191, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9970968961715698, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9970512390136719, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9970574975013733, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9970020055770874, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9969486594200134, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9969704151153564, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9970410466194153, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9969462156295776, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9968861937522888, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9969245791435242, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.9968387484550476, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.9969109296798706, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.996951699256897, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "39. Average loss:0.9968418478965759, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "40. Average loss:0.9968673586845398, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "41. Average loss:0.9969505667686462, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "42. Average loss:0.9968279600143433, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "43. Average loss:0.996819794178009, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "44. Average loss:0.9968057870864868, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 10:47:05,943]\u001b[0m Trial 1 finished with value: 0.5607059580284635 and parameters: {'learning_rate': 1.5e-05, 'epochs': 45, 'LEN_STRI': 230, 'hidden_dim': 246, 'n_layers': 2, 'dropout': 0.3049529915153013}. Best is trial 0 with value: 0.5607067137809187.\u001b[0m\n",
      "0. Average loss:1.0631643533706665, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "1. Average loss:1.0421785116195679, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:1.0254461765289307, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:1.009817123413086, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:1.0004220008850098, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9970952272415161, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9966972470283508, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9966984391212463, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9965377449989319, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9965655207633972, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9965035319328308, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9966136813163757, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9966011047363281, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9966661334037781, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9965598583221436, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9966685771942139, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9964321851730347, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9966368675231934, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9965055584907532, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9965219497680664, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9965407252311707, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9966359734535217, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9965187311172485, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9965044856071472, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9965388178825378, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9966652989387512, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9964344501495361, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9964970946311951, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9963784217834473, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9965751767158508, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9965459704399109, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9964208006858826, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9965797662734985, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9965243339538574, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9965256452560425, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9966102242469788, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.996671736240387, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.9965097308158875, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.9964333772659302, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "39. Average loss:0.9966164827346802, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "40. Average loss:0.9964761734008789, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "41. Average loss:0.9964286684989929, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "42. Average loss:0.9964801073074341, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "43. Average loss:0.9964295029640198, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "44. Average loss:0.9964383840560913, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 11:08:54,496]\u001b[0m Trial 2 finished with value: 0.5608470036826247 and parameters: {'learning_rate': 1.5e-05, 'epochs': 45, 'LEN_STRI': 159, 'hidden_dim': 97, 'n_layers': 3, 'dropout': 0.37422127143536166}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n",
      "0. Average loss:1.02797269821167, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "1. Average loss:1.0206246376037598, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:1.0147361755371094, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:1.0092408657073975, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:1.00418221950531, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:1.000635027885437, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9986488819122314, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9975795149803162, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9970444440841675, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9968596696853638, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9967241287231445, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.996676504611969, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9967016577720642, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.996691107749939, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9966196417808533, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9966204762458801, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9966056942939758, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9965834021568298, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.996562123298645, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9965989589691162, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9966006875038147, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9965243339538574, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9965351223945618, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9965591430664062, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9964965581893921, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9964786767959595, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9965026378631592, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9964715242385864, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9964724183082581, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9964711666107178, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9964325428009033, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9964227080345154, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9964390397071838, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.996408224105835, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9964221119880676, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9963625073432922, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.9963744878768921, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.996367335319519, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.9963452816009521, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "39. Average loss:0.9963861703872681, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "40. Average loss:0.9963667988777161, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "41. Average loss:0.9963405728340149, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "42. Average loss:0.9963061809539795, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 11:28:06,333]\u001b[0m Trial 3 finished with value: 0.5606444977716832 and parameters: {'learning_rate': 1.5e-05, 'epochs': 43, 'LEN_STRI': 130, 'hidden_dim': 100, 'n_layers': 2, 'dropout': 0.3188277250836966}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n",
      "0. Average loss:1.1046111583709717, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.0807055234909058, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "2. Average loss:1.0674885511398315, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "3. Average loss:1.0571069717407227, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "4. Average loss:1.0479274988174438, train_accuracy: 0.34,              val_accuracy: 0.55\n",
      "5. Average loss:1.0396860837936401, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:1.0323574542999268, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:1.0258893966674805, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:1.0202690362930298, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:1.0154248476028442, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:1.011319875717163, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:1.0078554153442383, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:1.0049524307250977, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:1.0024609565734863, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:1.000501275062561, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9991278648376465, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.998167097568512, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9975982308387756, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9972326159477234, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9969981908798218, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9968821406364441, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9967995285987854, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9967575669288635, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9967334866523743, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9967257380485535, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9966863393783569, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.996695876121521, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9966481924057007, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9966975450515747, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9966363906860352, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9966648817062378, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9966585636138916, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9966698884963989, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9966298341751099, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9966360926628113, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9966446161270142, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 11:45:49,635]\u001b[0m Trial 4 finished with value: 0.5604798788655291 and parameters: {'learning_rate': 1.5e-05, 'epochs': 36, 'LEN_STRI': 166, 'hidden_dim': 126, 'n_layers': 3, 'dropout': 0.2264793595438527}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n",
      "0. Average loss:1.0266456604003906, train_accuracy: 0.47,              val_accuracy: 0.56\n",
      "1. Average loss:1.0061200857162476, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9992027282714844, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9970961809158325, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9967849850654602, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.996688187122345, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9965803027153015, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9967390894889832, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9966920614242554, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9966685771942139, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9966411590576172, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9966610074043274, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9966720342636108, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9965838193893433, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9966438412666321, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9965887665748596, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9965638518333435, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9965876936912537, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9965330958366394, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9965704679489136, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9965275526046753, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9965603947639465, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.996537446975708, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9965386390686035, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9965946078300476, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9965153336524963, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 11:59:46,215]\u001b[0m Trial 5 finished with value: 0.5608446486653932 and parameters: {'learning_rate': 1.5e-05, 'epochs': 26, 'LEN_STRI': 147, 'hidden_dim': 247, 'n_layers': 3, 'dropout': 0.3767178069709639}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n",
      "0. Average loss:1.030387282371521, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "1. Average loss:1.0022718906402588, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9971498847007751, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9965067505836487, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9964014887809753, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9963878393173218, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9963682889938354, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9963555932044983, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9963644742965698, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9963505864143372, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9963551163673401, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9963620901107788, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9963536262512207, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9963520765304565, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9963463544845581, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9963386058807373, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9963439702987671, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9963428378105164, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9963452219963074, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9963329434394836, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9963390231132507, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9963294863700867, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9963386654853821, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9963188767433167, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963207840919495, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9963211417198181, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9963339567184448, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9963192343711853, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9963297843933105, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9963151812553406, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9963206648826599, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9963149428367615, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.996315062046051, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9963114857673645, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9963017702102661, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9963079690933228, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.9963029026985168, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.9962936639785767, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.9963154196739197, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 12:26:02,685]\u001b[0m Trial 6 finished with value: 0.5606916122992306 and parameters: {'learning_rate': 1.5e-05, 'epochs': 39, 'LEN_STRI': 83, 'hidden_dim': 228, 'n_layers': 2, 'dropout': 0.35160157251919166}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.29768581997529914 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.4587370157241821, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.4259846210479736, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "2. Average loss:1.4021154642105103, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "3. Average loss:1.3818612098693848, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "4. Average loss:1.3628004789352417, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "5. Average loss:1.3443305492401123, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "6. Average loss:1.3262861967086792, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "7. Average loss:1.3086881637573242, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "8. Average loss:1.2915120124816895, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "9. Average loss:1.2747526168823242, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "10. Average loss:1.258457064628601, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "11. Average loss:1.242565631866455, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "12. Average loss:1.227135419845581, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "13. Average loss:1.2121639251708984, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "14. Average loss:1.197674036026001, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "15. Average loss:1.1836780309677124, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "16. Average loss:1.1701443195343018, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "17. Average loss:1.157139539718628, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "18. Average loss:1.1446123123168945, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "19. Average loss:1.1326180696487427, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "20. Average loss:1.1212221384048462, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "21. Average loss:1.110360860824585, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "22. Average loss:1.1000134944915771, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "23. Average loss:1.0902280807495117, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "24. Average loss:1.0810012817382812, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "25. Average loss:1.072293996810913, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "26. Average loss:1.064134955406189, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "27. Average loss:1.0565565824508667, train_accuracy: 0.46,              val_accuracy: 0.56\n",
      "28. Average loss:1.0495414733886719, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:1.0430471897125244, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:1.0371153354644775, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:1.0317103862762451, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:1.02680504322052, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:1.0224177837371826, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:1.0185184478759766, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:1.0150401592254639, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:1.0120049715042114, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 12:35:26,684]\u001b[0m Trial 7 finished with value: 0.5608365019011406 and parameters: {'learning_rate': 1.5e-05, 'epochs': 37, 'LEN_STRI': 209, 'hidden_dim': 77, 'n_layers': 1, 'dropout': 0.29768581997529914}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3644146677270465 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.0586833953857422, train_accuracy: 0.31,              val_accuracy: 0.31\n",
      "1. Average loss:1.0201205015182495, train_accuracy: 0.46,              val_accuracy: 0.56\n",
      "2. Average loss:1.0002437829971313, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9978833198547363, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9974846243858337, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9972288608551025, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9970290064811707, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9968826770782471, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9967580437660217, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9966767430305481, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9966170191764832, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9965148568153381, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9964819550514221, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9964423179626465, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9963836669921875, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9963750839233398, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9963425993919373, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.996296763420105, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9962913393974304, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.996275007724762, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9962482452392578, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9962307214736938, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9962335228919983, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9961900115013123, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9961914420127869, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9961643218994141, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9961514472961426, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9961362481117249, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9961065053939819, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 12:58:50,897]\u001b[0m Trial 8 finished with value: 0.5607870839604001 and parameters: {'learning_rate': 1.5e-05, 'epochs': 29, 'LEN_STRI': 58, 'hidden_dim': 198, 'n_layers': 1, 'dropout': 0.3644146677270465}. Best is trial 2 with value: 0.5608470036826247.\u001b[0m\n",
      "0. Average loss:1.110739827156067, train_accuracy: 0.36,              val_accuracy: 0.56\n",
      "1. Average loss:1.02188241481781, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9976463913917542, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9965443015098572, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9965149760246277, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9964640140533447, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9964762926101685, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9964472651481628, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.996436357498169, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9964420199394226, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.996432900428772, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9964150786399841, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9964224100112915, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9964292645454407, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9963659644126892, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9964078664779663, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9963635802268982, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9963341951370239, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9963571429252625, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9963322877883911, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9963195323944092, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9962867498397827, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9963005185127258, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9962873458862305, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963016510009766, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9962916374206543, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9962730407714844, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9962966442108154, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9962611198425293, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 13:21:14,935]\u001b[0m Trial 9 finished with value: 0.5608565762703134 and parameters: {'learning_rate': 1.5e-05, 'epochs': 29, 'LEN_STRI': 73, 'hidden_dim': 214, 'n_layers': 2, 'dropout': 0.33644302650272623}. Best is trial 9 with value: 0.5608565762703134.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.06282933196473534 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.220055341720581, train_accuracy: 0.16,              val_accuracy: 0.17\n",
      "1. Average loss:1.1583545207977295, train_accuracy: 0.17,              val_accuracy: 0.29\n",
      "2. Average loss:1.0989450216293335, train_accuracy: 0.5,              val_accuracy: 0.56\n",
      "3. Average loss:1.0520843267440796, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:1.0229723453521729, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:1.007522702217102, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:1.0003730058670044, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9976767897605896, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9968289136886597, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9966037273406982, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9965417981147766, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9965267181396484, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9965164661407471, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9965025782585144, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9965120553970337, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9964857697486877, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9964839220046997, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.996476411819458, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9964675903320312, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9964694976806641, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.996468186378479, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9964518547058105, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9964398145675659, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9964409470558167, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9964383244514465, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9964423179626465, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9964379668235779, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9964272975921631, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9964182376861572, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.996433436870575, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.996418833732605, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9964109063148499, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 13:36:56,671]\u001b[0m Trial 10 finished with value: 0.560680606806068 and parameters: {'learning_rate': 1.5e-05, 'epochs': 32, 'LEN_STRI': 97, 'hidden_dim': 182, 'n_layers': 1, 'dropout': 0.06282933196473534}. Best is trial 9 with value: 0.5608565762703134.\u001b[0m\n",
      "0. Average loss:1.0899083614349365, train_accuracy: 0.48,              val_accuracy: 0.56\n",
      "1. Average loss:1.0470192432403564, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:1.0144703388214111, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:1.0004775524139404, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9973755478858948, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9969697594642639, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9968906044960022, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.996895432472229, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.996947169303894, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9969015717506409, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9968729615211487, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9968819618225098, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9968715906143188, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.996821403503418, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9968133568763733, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9967817664146423, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9967727661132812, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9968471527099609, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9968218803405762, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9968340992927551, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9967774152755737, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9968080520629883, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9967792630195618, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.996728777885437, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9967465996742249, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9967420101165771, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9967479109764099, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9967816472053528, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9966868162155151, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9967035055160522, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9967184066772461, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9967254996299744, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9966364502906799, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9966797828674316, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9967162013053894, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9966782927513123, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.9967089295387268, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.9966954588890076, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.996638834476471, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "39. Average loss:0.9966945052146912, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "40. Average loss:0.996667206287384, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "41. Average loss:0.9966404438018799, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "42. Average loss:0.9966493248939514, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "43. Average loss:0.9966305494308472, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "44. Average loss:0.9966447353363037, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "45. Average loss:0.9966983199119568, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "46. Average loss:0.99663907289505, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "47. Average loss:0.9966709017753601, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "48. Average loss:0.9966940879821777, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "49. Average loss:0.9965924024581909, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 14:01:22,492]\u001b[0m Trial 11 finished with value: 0.5607686772994364 and parameters: {'learning_rate': 1.5e-05, 'epochs': 50, 'LEN_STRI': 171, 'hidden_dim': 50, 'n_layers': 3, 'dropout': 0.24016147336673965}. Best is trial 9 with value: 0.5608565762703134.\u001b[0m\n",
      "0. Average loss:1.1518027782440186, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.0560301542282104, train_accuracy: 0.38,              val_accuracy: 0.55\n",
      "2. Average loss:1.0036768913269043, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9988020658493042, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.998440146446228, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9980860948562622, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9979272484779358, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9977015852928162, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9974541068077087, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9973325133323669, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9972934126853943, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9971585869789124, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.997069239616394, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9969539642333984, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9969282746315002, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9969047904014587, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9968297481536865, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9967914819717407, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.996772050857544, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9967302680015564, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9966292977333069, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.996658205986023, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9966217279434204, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9966405630111694, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9965568780899048, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9965339303016663, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9965618848800659, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9965291023254395, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9965633153915405, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9965568780899048, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9964693784713745, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.996482253074646, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 14:22:06,799]\u001b[0m Trial 12 finished with value: 0.560869207342168 and parameters: {'learning_rate': 1.5e-05, 'epochs': 32, 'LEN_STRI': 117, 'hidden_dim': 145, 'n_layers': 3, 'dropout': 0.3972202256656575}. Best is trial 12 with value: 0.560869207342168.\u001b[0m\n",
      "0. Average loss:1.0986860990524292, train_accuracy: 0.38,              val_accuracy: 0.56\n",
      "1. Average loss:1.03965163230896, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:1.0095157623291016, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9993326663970947, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9970853924751282, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9966627359390259, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.996631920337677, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9965240359306335, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9965481162071228, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.996481716632843, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9965145587921143, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9964459538459778, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9964812994003296, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9964657425880432, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9964684844017029, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.996429443359375, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9964693188667297, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9964503049850464, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9964478611946106, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9964205026626587, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9964215755462646, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9964146018028259, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9963851571083069, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9963820576667786, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963797926902771, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9963275790214539, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9963858723640442, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9963858127593994, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9963490962982178, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9963353276252747, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9963581562042236, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9963184595108032, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 14:41:27,559]\u001b[0m Trial 13 finished with value: 0.5606849627489737 and parameters: {'learning_rate': 1.5e-05, 'epochs': 32, 'LEN_STRI': 108, 'hidden_dim': 154, 'n_layers': 2, 'dropout': 0.39739569855223816}. Best is trial 12 with value: 0.560869207342168.\u001b[0m\n",
      "0. Average loss:1.0980069637298584, train_accuracy: 0.36,              val_accuracy: 0.56\n",
      "1. Average loss:0.9981403350830078, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9968127608299255, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9967553019523621, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.996616780757904, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9965696334838867, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9965232014656067, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9964643120765686, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.996431827545166, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9963507652282715, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9963809847831726, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9963233470916748, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9963282346725464, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9963526725769043, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9962670803070068, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9962947368621826, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9962648749351501, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9962705969810486, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9962790012359619, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9962691068649292, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9962278008460999, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9962355494499207, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9962278604507446, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9962372183799744, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.996224582195282, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 15:17:30,383]\u001b[0m Trial 14 finished with value: 0.5608378636497209 and parameters: {'learning_rate': 1.5e-05, 'epochs': 25, 'LEN_STRI': 52, 'hidden_dim': 160, 'n_layers': 3, 'dropout': 0.2708171830576327}. Best is trial 12 with value: 0.560869207342168.\u001b[0m\n",
      "0. Average loss:1.0235286951065063, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "1. Average loss:1.0122528076171875, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:1.0048757791519165, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:1.0006256103515625, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9986435174942017, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9976888298988342, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9972354173660278, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9968799352645874, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9966890215873718, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.996629536151886, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9966004490852356, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9965842366218567, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9966093897819519, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9965452551841736, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9964926242828369, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9963995218276978, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.996542751789093, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9964289665222168, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.996513307094574, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.996482789516449, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9964587092399597, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9965124130249023, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9965337514877319, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9965130686759949, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.996436357498169, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9964156150817871, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9963943362236023, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9963673949241638, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9964457154273987, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9964452981948853, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9963783621788025, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 15:30:22,641]\u001b[0m Trial 15 finished with value: 0.5607758711665506 and parameters: {'learning_rate': 1.5e-05, 'epochs': 31, 'LEN_STRI': 120, 'hidden_dim': 193, 'n_layers': 2, 'dropout': 0.16844013209687955}. Best is trial 12 with value: 0.560869207342168.\u001b[0m\n",
      "0. Average loss:1.065413475036621, train_accuracy: 0.49,              val_accuracy: 0.56\n",
      "1. Average loss:1.008140206336975, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9970219731330872, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.996521532535553, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9965178370475769, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9965101480484009, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9964700937271118, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9964722990989685, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9964667558670044, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9964585304260254, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9964532852172852, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9964590668678284, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9964426755905151, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9964339137077332, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9964147210121155, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.996369481086731, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9964075088500977, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.996368944644928, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9963794350624084, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.996372640132904, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9963693618774414, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9963583946228027, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9963945150375366, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9963428974151611, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963258504867554, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.996338427066803, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9963283538818359, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9963443875312805, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9963201284408569, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 15:56:25,356]\u001b[0m Trial 16 finished with value: 0.5608455908276806 and parameters: {'learning_rate': 1.5e-05, 'epochs': 29, 'LEN_STRI': 80, 'hidden_dim': 140, 'n_layers': 3, 'dropout': 0.3281184637156334}. Best is trial 12 with value: 0.560869207342168.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.39892437897340766 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.0039266347885132, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "1. Average loss:0.9983824491500854, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9971010684967041, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9968110918998718, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9967228770256042, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9966606497764587, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9966310262680054, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9965683817863464, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.996563196182251, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9965283870697021, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9964948892593384, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9964794516563416, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9964412450790405, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9964510202407837, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.99643474817276, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9964136481285095, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9964100122451782, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9963972568511963, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.996354341506958, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9963510036468506, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9963316917419434, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9963472485542297, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9963252544403076, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.996296226978302, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963175058364868, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9963067770004272, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9962883591651917, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9962722063064575, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9962700009346008, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9962501525878906, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9962542653083801, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.996246337890625, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9962019324302673, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9962160587310791, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9961937069892883, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 16:16:22,634]\u001b[0m Trial 17 finished with value: 0.560869978413457 and parameters: {'learning_rate': 1.5e-05, 'epochs': 35, 'LEN_STRI': 77, 'hidden_dim': 216, 'n_layers': 1, 'dropout': 0.39892437897340766}. Best is trial 17 with value: 0.560869978413457.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.27657548577588714 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.2197580337524414, train_accuracy: 0.16,              val_accuracy: 0.17\n",
      "1. Average loss:1.176213026046753, train_accuracy: 0.16,              val_accuracy: 0.17\n",
      "2. Average loss:1.1277614831924438, train_accuracy: 0.18,              val_accuracy: 0.29\n",
      "3. Average loss:1.0791940689086914, train_accuracy: 0.47,              val_accuracy: 0.56\n",
      "4. Average loss:1.0381805896759033, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:1.0121856927871704, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:1.0010768175125122, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9982032179832458, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9976127743721008, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9974488615989685, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9973477721214294, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.997292160987854, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9972330331802368, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9971923232078552, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.997128427028656, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9970932006835938, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9970555901527405, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9970108866691589, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9969676733016968, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9969527125358582, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9968954920768738, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9968816041946411, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9968565702438354, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9968346357345581, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9967989921569824, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9967857599258423, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9967703819274902, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.996746301651001, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9967231154441833, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.996702253818512, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9966943860054016, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9966899752616882, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9966620206832886, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9966549873352051, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9966443181037903, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 16:27:12,006]\u001b[0m Trial 18 finished with value: 0.5605130796813708 and parameters: {'learning_rate': 1.5e-05, 'epochs': 35, 'LEN_STRI': 137, 'hidden_dim': 179, 'n_layers': 1, 'dropout': 0.27657548577588714}. Best is trial 17 with value: 0.560869978413457.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1734341001480769 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.1469342708587646, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.083331823348999, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "2. Average loss:1.0323220491409302, train_accuracy: 0.39,              val_accuracy: 0.56\n",
      "3. Average loss:1.0063825845718384, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9991746544837952, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9974188804626465, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9973618984222412, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9971915483474731, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9967427253723145, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9966884255409241, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9969818592071533, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.996941864490509, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9965797066688538, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9965474605560303, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.996538519859314, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9966418147087097, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9966410994529724, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9966211915016174, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9967894554138184, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9964514374732971, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9964261651039124, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9967400431632996, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9965523481369019, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9963823556900024, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963743686676025, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9963670969009399, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9965094923973083, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.996342658996582, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9966714978218079, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9963330626487732, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9963210225105286, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9963132739067078, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9963026642799377, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9964460730552673, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "34. Average loss:0.9962881803512573, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "35. Average loss:0.9966220259666443, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "36. Average loss:0.9962775707244873, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "37. Average loss:0.9965946674346924, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "38. Average loss:0.9964158535003662, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "39. Average loss:0.9962570667266846, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "40. Average loss:0.9964050054550171, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 16:41:31,420]\u001b[0m Trial 19 finished with value: 0.5607791446983011 and parameters: {'learning_rate': 1.5e-05, 'epochs': 41, 'LEN_STRI': 104, 'hidden_dim': 117, 'n_layers': 1, 'dropout': 0.1734341001480769}. Best is trial 17 with value: 0.560869978413457.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.39590454056790075 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.1547690629959106, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.1184301376342773, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "2. Average loss:1.0869741439819336, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "3. Average loss:1.0629922151565552, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "4. Average loss:1.0498623847961426, train_accuracy: 0.31,              val_accuracy: 0.3\n",
      "5. Average loss:1.0398955345153809, train_accuracy: 0.38,              val_accuracy: 0.46\n",
      "6. Average loss:1.031242847442627, train_accuracy: 0.49,              val_accuracy: 0.55\n",
      "7. Average loss:1.0238126516342163, train_accuracy: 0.52,              val_accuracy: 0.56\n",
      "8. Average loss:1.0176421403884888, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:1.0126253366470337, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:1.0086619853973389, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:1.0055372714996338, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:1.00316321849823, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:1.0014324188232422, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:1.0001558065414429, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.999250590801239, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9985995292663574, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9981715083122253, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9978275299072266, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9976211786270142, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9974501132965088, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9973453879356384, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9972244501113892, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.9971641898155212, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9971097707748413, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9970570802688599, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9970400929450989, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.9970003962516785, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "28. Average loss:0.9969640970230103, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "29. Average loss:0.9969461560249329, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "30. Average loss:0.9969186186790466, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "31. Average loss:0.9969117045402527, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "32. Average loss:0.9969497919082642, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "33. Average loss:0.9969258904457092, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 16:51:22,979]\u001b[0m Trial 20 finished with value: 0.5605645603067703 and parameters: {'learning_rate': 1.5e-05, 'epochs': 34, 'LEN_STRI': 184, 'hidden_dim': 162, 'n_layers': 1, 'dropout': 0.39590454056790075}. Best is trial 17 with value: 0.560869978413457.\u001b[0m\n",
      "0. Average loss:1.0929663181304932, train_accuracy: 0.37,              val_accuracy: 0.56\n",
      "1. Average loss:1.01466703414917, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "2. Average loss:0.9983094930648804, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9969643354415894, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.996733546257019, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9966377019882202, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.996642529964447, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.99662184715271, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9965786337852478, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.996574878692627, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.9964965581893921, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9964427351951599, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9964290261268616, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "13. Average loss:0.9964566826820374, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "14. Average loss:0.9963759183883667, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "15. Average loss:0.9964038133621216, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "16. Average loss:0.9963772296905518, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "17. Average loss:0.9963667988777161, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "18. Average loss:0.9963476061820984, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "19. Average loss:0.9963244199752808, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "20. Average loss:0.9963288307189941, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "21. Average loss:0.9963341951370239, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "22. Average loss:0.9963205456733704, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "23. Average loss:0.99631267786026, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "24. Average loss:0.9963012933731079, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "25. Average loss:0.9963236451148987, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "26. Average loss:0.9962875247001648, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "27. Average loss:0.996259331703186, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[32m[I 2023-02-16 17:12:05,707]\u001b[0m Trial 21 finished with value: 0.5608635497428668 and parameters: {'learning_rate': 1.5e-05, 'epochs': 28, 'LEN_STRI': 74, 'hidden_dim': 218, 'n_layers': 2, 'dropout': 0.3380610544549609}. Best is trial 17 with value: 0.560869978413457.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gleb/anaconda3/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.34084287352088155 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Average loss:1.0775705575942993, train_accuracy: 0.31,              val_accuracy: 0.27\n",
      "1. Average loss:1.0269469022750854, train_accuracy: 0.42,              val_accuracy: 0.56\n",
      "2. Average loss:1.0067371129989624, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "3. Average loss:0.9996345639228821, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "4. Average loss:0.9973170161247253, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "5. Average loss:0.9966353178024292, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "6. Average loss:0.9964052438735962, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "7. Average loss:0.9963167905807495, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "8. Average loss:0.9962854981422424, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "9. Average loss:0.9962741732597351, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "10. Average loss:0.996270477771759, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "11. Average loss:0.9962632656097412, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "12. Average loss:0.9962645769119263, train_accuracy: 0.53,              val_accuracy: 0.56\n",
      "\u001b[33m[W 2023-02-16 17:20:30,347]\u001b[0m Trial 22 failed with parameters: {'learning_rate': 1.5e-05, 'epochs': 27, 'LEN_STRI': 68, 'hidden_dim': 215, 'n_layers': 1, 'dropout': 0.34084287352088155} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gleb/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_29493/2804601761.py\", line 14, in objective\n",
      "    accuracy = train_and_evaluet(params)\n",
      "  File \"/tmp/ipykernel_29493/191883759.py\", line 13, in train_and_evaluet\n",
      "    = train_model(model, train_loader, val_loader, loss_func, opt, param['epochs'])\n",
      "  File \"/tmp/ipykernel_29493/2621523198.py\", line 24, in train_model\n",
      "    loss_value.backward()\n",
      "  File \"/home/gleb/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/gleb/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-02-16 17:20:30,351]\u001b[0m Trial 22 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29493/3069494236.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29493/2804601761.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     12\u001b[0m           }\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29493/191883759.py\u001b[0m in \u001b[0;36mtrain_and_evaluet\u001b[0;34m(param)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_history\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_29493/2621523198.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=10000, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a86a36-5212-4507-bc9a-6860a490b79a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
